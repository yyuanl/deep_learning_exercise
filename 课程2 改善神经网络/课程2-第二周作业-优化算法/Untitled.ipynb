{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import math\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import opt_utils\n",
    "import testCase\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (7.0, 4.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearnest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_gd(parameters, grads, learning_rate):\n",
    "    num_layers = len(parameters) // 2\n",
    "    for i in range(num_layers):\n",
    "        parameters[\"W\" + str(i + 1)] = parameters[\"W\" + str(i + 1)] - learning_rate * grads[\"dW\" + str(i + 1)]\n",
    "        parameters[\"b\" + str(i + 1)] = parameters[\"b\" + str(i + 1)] - learning_rate * grads[\"db\" + str(i + 1)]\n",
    "    \n",
    "    return parameters\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #测试update_parameters_with_gd\n",
    "# print(\"-------------测试update_parameters_with_gd-------------\")\n",
    "# parameters , grads , learning_rate = testCase.update_parameters_with_gd_test_case()\n",
    "# parameters = update_parameters_with_gd(parameters,grads,learning_rate)\n",
    "# print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "# print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "# print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "# print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batches_size = 64, seed = 0):\n",
    "    np.random.seed(seed)\n",
    "    mini_batches = []\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    permutation = list(np.random.permutation(m))  # 把0到m-1的自然数打乱，放入list\n",
    "    X = X[:, permutation] # 列按照打乱来重新排\n",
    "    Y = Y[:, permutation]\n",
    "    \n",
    "    num_batch = math.floor(m / mini_batch_size)\n",
    "    for i in range(num_batch):\n",
    "        # 取第i + 1个batch\n",
    "        X_batch_i = X[:, i * mini_batches_size:(i + 1) * mini_batches_size]\n",
    "        Y_batch_i = Y[:, i * mini_batches_size:(i + 1) * mini_batches_size]\n",
    "        # 把第i + 1个batch放到列表里\n",
    "        mini_batches.append((X_batch_i, Y_batch_i))\n",
    "        \n",
    "    if m % mini_batches_size != 0:\n",
    "        X_batch_last = X[:, num_batch * mini_batches_size:]\n",
    "        Y_batch_last = Y[:, num_batch * mini_batches_size:]\n",
    "        \n",
    "        mini_batches.append((X_batch_last, Y_batch_last))  \n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #测试random_mini_batches\n",
    "# print(\"-------------测试random_mini_batches-------------\")\n",
    "# X_assess,Y_assess,mini_batch_size = testCase.random_mini_batches_test_case()\n",
    "# mini_batches = random_mini_batches(X_assess,Y_assess,mini_batch_size)\n",
    "\n",
    "# print(\"第1个mini_batch_X 的维度为：\",mini_batches[0][0].shape)\n",
    "# print(\"第1个mini_batch_Y 的维度为：\",mini_batches[0][1].shape)\n",
    "# print(\"第2个mini_batch_X 的维度为：\",mini_batches[1][0].shape)\n",
    "# print(\"第2个mini_batch_Y 的维度为：\",mini_batches[1][1].shape)\n",
    "# print(\"第3个mini_batch_X 的维度为：\",mini_batches[2][0].shape)\n",
    "# print(\"第3个mini_batch_Y 的维度为：\",mini_batches[2][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_velocity(parameters):\n",
    "    num_layers = len(parameters) // 2\n",
    "    v = {}\n",
    "    for i in range(num_layers):\n",
    "        v[\"dW\" + str(i + 1)] = np.zeros_like(parameters[\"W\" + str(i + 1)])\n",
    "        v[\"db\" + str(i + 1)] = np.zeros_like(parameters[\"b\" + str(i + 1)])\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #测试initialize_velocity\n",
    "# print(\"-------------测试initialize_velocity-------------\")\n",
    "# parameters = testCase.initialize_velocity_test_case()\n",
    "# v = initialize_velocity(parameters)\n",
    "\n",
    "# print('v[\"dW1\"] = ' + str(v[\"dW1\"]))\n",
    "# print('v[\"db1\"] = ' + str(v[\"db1\"]))\n",
    "# print('v[\"dW2\"] = ' + str(v[\"dW2\"]))\n",
    "# print('v[\"db2\"] = ' + str(v[\"db2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_momentun(parameters,grads,v,beta,learning_rate):\n",
    "    \"\"\"\n",
    "    更新一次每层参数，用动量梯度下降\n",
    "    调用次函数j次，就更新参数j次，v其实是另一种梯度下降方向，普通下降方向的就是当前参数W的梯度，现在是考虑了之前所有的梯度不仅仅是当前的\n",
    "    \"\"\"\n",
    "    num_layers = len(parameters)//2\n",
    "    for i in range(num_layers):\n",
    "        # 更新一次第i + 1层动量及其参数\n",
    "        v[\"dW\" + str(i + 1)] = beta * v[\"dW\" + str(i + 1)] + (1 - beta) * grads[\"dW\" + str(i + 1)]\n",
    "        v[\"db\" + str(i + 1)] = beta * v[\"db\" + str(i + 1)] + (1 - beta) * grads[\"db\" + str(i + 1)]\n",
    "        parameters[\"W\" + str(i + 1)] = parameters[\"W\" + str(i + 1)] - learning_rate * v[\"dW\" + str(i + 1)]\n",
    "        parameters[\"b\" + str(i + 1)] = parameters[\"b\" + str(i + 1)] - learning_rate * v[\"db\" + str(i + 1)]\n",
    "    \n",
    "    return parameters,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_adam(parameters):\n",
    "\n",
    "    L = len(parameters) // 2\n",
    "    v = {}\n",
    "    s = {}\n",
    "\n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"W\" + str(l + 1)])\n",
    "        v[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l + 1)])\n",
    "\n",
    "        s[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"W\" + str(l + 1)])\n",
    "        s[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l + 1)])\n",
    "\n",
    "    return (v,s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试initialize_adam\n",
    "print(\"-------------测试initialize_adam-------------\")\n",
    "parameters = testCase.initialize_adam_test_case()\n",
    "v,s = initialize_adam(parameters)\n",
    "\n",
    "print('v[\"dW1\"] = ' + str(v[\"dW1\"])) \n",
    "print('v[\"db1\"] = ' + str(v[\"db1\"])) \n",
    "print('v[\"dW2\"] = ' + str(v[\"dW2\"])) \n",
    "print('v[\"db2\"] = ' + str(v[\"db2\"])) \n",
    "print('s[\"dW1\"] = ' + str(s[\"dW1\"])) \n",
    "print('s[\"db1\"] = ' + str(s[\"db1\"])) \n",
    "print('s[\"dW2\"] = ' + str(s[\"dW2\"])) \n",
    "print('s[\"db2\"] = ' + str(s[\"db2\"])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6(py36)",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
