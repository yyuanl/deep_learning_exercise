{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\jupyter_notebook\\deep_learning_exercise\\课程2 改善神经网络\\课程2-第一周作业-初始化、正则化、梯度校验\\reg_utils.py:61: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(parameters['W' + str(l)].shape == layer_dims[l], layer_dims[l-1])\n",
      "D:\\jupyter_notebook\\deep_learning_exercise\\课程2 改善神经网络\\课程2-第一周作业-初始化、正则化、梯度校验\\reg_utils.py:62: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(parameters['W' + str(l)].shape == layer_dims[l], 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import init_utils\n",
    "import reg_utils\n",
    "import gc_utils\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (7, 4)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y, test_X, test_Y = init_utils.load_dataset(is_plot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_zeros(layers_dims):\n",
    "    # layers_dims包括输入层\n",
    "    parameters = {}\n",
    "    num_layers = len(layers_dims)\n",
    "    for i in range(1, num_layers):\n",
    "        parameters[\"W\" + str(i)] = np.zeros((layers_dims[i], layers_dims[i - 1]))\n",
    "        parameters[\"b\" + str(i)] = np.zeros((layers_dims[i], 1))\n",
    "        assert(parameters[\"W\" + str(i)].shape == (layers_dims[i], layers_dims[i - 1]))\n",
    "    return parameters\n",
    "\n",
    "def initialize_parameters_random(layers_dims):\n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    num_layers = len(layers_dims)\n",
    "    for i in range(1, num_layers):\n",
    "        parameters[\"W\" + str(i)] = np.random.randn(layers_dims[i], layers_dims[i - 1]) * 10\n",
    "        parameters[\"b\" + str(i)] = np.zeros((layers_dims[i], 1))\n",
    "        assert(parameters[\"W\" + str(i)].shape == (layers_dims[i], layers_dims[i - 1]))\n",
    "    return parameters\n",
    "# def initialize_parameters_random(layers_dims):\n",
    "#     \"\"\"\n",
    "#     参数：\n",
    "#         layers_dims - 列表，模型的层数和对应每一层的节点的数量\n",
    "#     返回\n",
    "#         parameters - 包含了所有W和b的字典\n",
    "#             W1 - 权重矩阵，维度为（layers_dims[1], layers_dims[0]）\n",
    "#             b1 - 偏置向量，维度为（layers_dims[1],1）\n",
    "#             ···\n",
    "#             WL - 权重矩阵，维度为（layers_dims[L], layers_dims[L -1]）\n",
    "#             b1 - 偏置向量，维度为（layers_dims[L],1）\n",
    "#     \"\"\"\n",
    "\n",
    "#     np.random.seed(3)               # 指定随机种子\n",
    "#     parameters = {}\n",
    "#     L = len(layers_dims)            # 层数\n",
    "\n",
    "#     for l in range(1, L):\n",
    "#         parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l - 1]) * 10 #使用10倍缩放\n",
    "#         parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n",
    "\n",
    "#         #使用断言确保我的数据格式是正确的\n",
    "#         assert(parameters[\"W\" + str(l)].shape == (layers_dims[l],layers_dims[l-1]))\n",
    "#         assert(parameters[\"b\" + str(l)].shape == (layers_dims[l],1))\n",
    "\n",
    "#     return parameters\n",
    "\n",
    "\n",
    "def initialize_parameters_he(layers_dims):\n",
    "    parameters = {}\n",
    "    np.random.seed(1)\n",
    "    num_layers = len(layers_dims)\n",
    "    for i in range(1, num_layers):\n",
    "        parameters[\"W\" + str(i)] = np.random.randn(layers_dims[i], layers_dims[i - 1]) * np.sqrt(2 / layers_dims[i - 1])\n",
    "        parameters[\"b\" + str(i)] = np.zeros((layers_dims[i], 1))\n",
    "        assert(parameters[\"W\" + str(i)].shape == (layers_dims[i], layers_dims[i - 1]))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters_zeros([3,2,1])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "\n",
    "parameters = initialize_parameters_random([3, 2, 1])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "\n",
    "parameters = initialize_parameters_he([2, 4, 1])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, learning_rate = 0.01, num_iterations = 15000, print_cost = True, initialization = \"he\", is_polt = True):\n",
    "    costs = []\n",
    "    layers_dims = [X.shape[0], 10, 5, 1]\n",
    "    \n",
    "    if initialization == \"zeros\":\n",
    "        parameters = initialize_parameters_zeros(layers_dims)\n",
    "    elif initialization == \"random\":\n",
    "        parameters = initialize_parameters_random(layers_dims)\n",
    "    elif initialization == \"he\":\n",
    "        parameters = initialize_parameters_he(layers_dims)\n",
    "    else:\n",
    "        print('initializing occurs error and program exits')\n",
    "        exit\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        a3, cache = init_utils.forward_propagation(X, parameters)\n",
    "        cost = init_utils.compute_loss(a3, Y)\n",
    "        grads = init_utils.backward_propagation(X, Y, cache)\n",
    "        parameters = init_utils.update_parameters(parameters, grads, learning_rate)\n",
    "        if i % 1000 == 0:\n",
    "            costs.append(cost)\n",
    "            if print_cost:\n",
    "                print(\"Iterations: {}\\t|cost:{}\".format(i, cost))\n",
    "    if is_polt:\n",
    "        plt.plot(costs)\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations')\n",
    "        plt.title(\"learning rate :\" + str(learning_rate))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters = model(train_X, train_Y, initialization = \"zeros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (\"训练集:\")\n",
    "# predictions_train = init_utils.predict(train_X, train_Y, parameters)\n",
    "# print (\"测试集:\")\n",
    "# predictions_test = init_utils.predict(test_X, test_Y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters = model(train_X, train_Y, initialization = \"random\",is_polt=True)\n",
    "# print(\"训练集：\")\n",
    "# predictions_train = init_utils.predict(train_X, train_Y, parameters)\n",
    "# print(\"测试集：\")\n",
    "# predictions_test = init_utils.predict(test_X, test_Y, parameters)\n",
    "\n",
    "# print(predictions_train)\n",
    "# print(predictions_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = model(train_X, train_Y, initialization = \"he\",is_polt=True)\n",
    "print(\"训练集:\")\n",
    "predictions_train = init_utils.predict(train_X, train_Y, parameters)\n",
    "print(\"测试集:\")\n",
    "init_utils.predictions_test = init_utils.predict(test_X, test_Y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Model with He initialization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5, 1.5])\n",
    "axes.set_ylim([-1.5, 1.5])\n",
    "init_utils.plot_decision_boundary(lambda x: init_utils.predict_dec(parameters, x.T), train_X, train_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y, test_X, test_Y = reg_utils.load_2D_dataset(is_plot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwad_propagation_with_dropout(X, parameters, keep_prob):\n",
    "    \"\"\"\n",
    "    cache = (Z1 , A1, W1, b1 ....)\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    w1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    w2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    w3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    Z1 = np.dot(w1, X) + b1\n",
    "    A1 = reg_utils.relu(Z1)\n",
    "    # 执行dropout\n",
    "    D1 = np.random.rand(A1.shape[0], A1.shape[1])\n",
    "    D1 = D1 < keep_drop\n",
    "    A1 = A1 * D1\n",
    "    A1 = A1 / keep_shape\n",
    "    \n",
    "    Z2 = np.dot(w2, A1) + b2\n",
    "    A2 = reg_utils.relu(Z2)\n",
    "    # 执行dropout\n",
    "    D2 = np.random.rand(A2.shape[0],A2.shape[1])\n",
    "    D2 = D2 < keep_drop\n",
    "    A2 = A2 * D2\n",
    "    A2 = A2 / keep_drop\n",
    "    \n",
    "    Z3 = np.dot(w2, A2) + b2\n",
    "    A3 = reg_utils.relu(Z3)\n",
    "    \n",
    "    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, D3, A3, W3, b3)\n",
    "    \n",
    "    return A3, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, learning_rate = 0.3, num_iterations = 30000, print_cost = True, is_plot = True, lambd = 0, keep_prob = 1):\n",
    "    costs = []\n",
    "    layers_dims = [X.shape[0], 20, 3, 1]\n",
    "    parameters = reg_utils.initialize_parameters(layers_dims)\n",
    "    for i in range(num_iterations):\n",
    "        #正向传播\n",
    "        if keep_prob == 1:\n",
    "            A_output, cache = reg_utils.forward_propagation(X, parameters)\n",
    "        elif keep_prob < 1:\n",
    "            A_output, cache = forwad_propagation_with_dropout(X, parameters, dropout, keep_prob)  # dropout\n",
    "        else :\n",
    "            print(\"keep_prob error\")\n",
    "            exit\n",
    "        # 计算loss\n",
    "        if lambd == 0:\n",
    "            cost.reg_utils.compute_cost(A_output, parameters)\n",
    "        else:\n",
    "            cost = compute_cost_with_regularization(A_output, parameters, lambd)\n",
    "        \n",
    "        #反向传播\n",
    "        if (lambd == 0 or keep_prob == 1):\n",
    "            #不正则化，也不使用dropout\n",
    "            grads = reg_utils.backward_propagation(X, Y, cache)\n",
    "        elif lambd != 0:\n",
    "            # 只正则化\n",
    "            grads = backward_propagation_with_regularization(A_output, Y, cache, lambd)\n",
    "        elif keep_prob < 1:\n",
    "            grads = back_propagation_with_dropout(A_outpout, Y, cache, keep_prob)\n",
    "        \n",
    "        parameters = reg_utils.update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            costs.append(cost)\n",
    "            if print_cost:\n",
    "                print(\"iteration:{}\\t | cost: {}\".format(i, cost))\n",
    "                \n",
    "    if is_plot:\n",
    "        plt.plot(costs)\n",
    "        plt.ylabel(\"cost\")\n",
    "        plt.xlabel(\"iterations\")\n",
    "        plt.title(\"learning rate :\" + str(learning_rate))\n",
    "        plt.show()\n",
    "    \n",
    "    return parameters  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17933259, 0.96776863, 0.22568321],\n",
       "       [0.71982196, 0.38965777, 0.5152401 ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.rand(2, 3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False,  True, False],\n",
       "       [ True, False,  True]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = a < 0.5\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6(py36)",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
