{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import init_utils\n",
    "import reg_utils\n",
    "import gc_utils\n",
    "import testCases\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (7, 4)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y, test_X, test_Y = init_utils.load_dataset(is_plot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "三种初始化参数对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_parameters_zeros(layers_dims):\n",
    "    # layers_dims包括输入层\n",
    "    parameters = {}\n",
    "    num_layers = len(layers_dims)\n",
    "    for i in range(1, num_layers):\n",
    "        parameters[\"W\" + str(i)] = np.zeros((layers_dims[i], layers_dims[i - 1]))\n",
    "        parameters[\"b\" + str(i)] = np.zeros((layers_dims[i], 1))\n",
    "        assert(parameters[\"W\" + str(i)].shape == (layers_dims[i], layers_dims[i - 1]))\n",
    "    return parameters\n",
    "\n",
    "def initialize_parameters_random(layers_dims):\n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    num_layers = len(layers_dims)\n",
    "    for i in range(1, num_layers):\n",
    "        parameters[\"W\" + str(i)] = np.random.randn(layers_dims[i], layers_dims[i - 1]) * 10\n",
    "        parameters[\"b\" + str(i)] = np.zeros((layers_dims[i], 1))\n",
    "        assert(parameters[\"W\" + str(i)].shape == (layers_dims[i], layers_dims[i - 1]))\n",
    "    return parameters\n",
    "\n",
    "def initialize_parameters_he(layers_dims):\n",
    "    parameters = {}\n",
    "    np.random.seed(1)\n",
    "    num_layers = len(layers_dims)\n",
    "    for i in range(1, num_layers):\n",
    "        parameters[\"W\" + str(i)] = np.random.randn(layers_dims[i], layers_dims[i - 1]) * np.sqrt(2 / layers_dims[i - 1])\n",
    "        parameters[\"b\" + str(i)] = np.zeros((layers_dims[i], 1))\n",
    "        assert(parameters[\"W\" + str(i)].shape == (layers_dims[i], layers_dims[i - 1]))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters_zeros([3,2,1])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "\n",
    "parameters = initialize_parameters_random([3, 2, 1])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "\n",
    "parameters = initialize_parameters_he([2, 4, 1])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, learning_rate = 0.01, num_iterations = 15000, print_cost = True, initialization = \"he\", is_polt = True):\n",
    "    costs = []\n",
    "    layers_dims = [X.shape[0], 10, 5, 1]\n",
    "    \n",
    "    if initialization == \"zeros\":\n",
    "        parameters = initialize_parameters_zeros(layers_dims)\n",
    "    elif initialization == \"random\":\n",
    "        parameters = initialize_parameters_random(layers_dims)\n",
    "    elif initialization == \"he\":\n",
    "        parameters = initialize_parameters_he(layers_dims)\n",
    "    else:\n",
    "        print('initializing occurs error and program exits')\n",
    "        exit\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        a3, cache = init_utils.forward_propagation(X, parameters)\n",
    "        cost = init_utils.compute_loss(a3, Y)\n",
    "        grads = init_utils.backward_propagation(X, Y, cache)\n",
    "        parameters = init_utils.update_parameters(parameters, grads, learning_rate)\n",
    "        if i % 1000 == 0:\n",
    "            costs.append(cost)\n",
    "            if print_cost:\n",
    "                print(\"Iterations: {}\\t|cost:{}\".format(i, cost))\n",
    "    if is_polt:\n",
    "        plt.plot(costs)\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations')\n",
    "        plt.title(\"learning rate :\" + str(learning_rate))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化为0\n",
    "parameters = model(train_X, train_Y, initialization = \"zeros\")\n",
    "print (\"训练集:\")\n",
    "predictions_train = init_utils.predict(train_X, train_Y, parameters)\n",
    "print (\"测试集:\")\n",
    "predictions_test = init_utils.predict(test_X, test_Y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#随机初始化\n",
    "parameters = model(train_X, train_Y, initialization = \"random\",is_polt=True)\n",
    "print(\"训练集：\")\n",
    "predictions_train = init_utils.predict(train_X, train_Y, parameters)\n",
    "print(\"测试集：\")\n",
    "predictions_test = init_utils.predict(test_X, test_Y, parameters)\n",
    "\n",
    "print(predictions_train)\n",
    "print(predictions_test)\n",
    "\n",
    "#随机初始化边界图\n",
    "plt.title(\"Model with large random initialization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5, 1.5])\n",
    "axes.set_ylim([-1.5, 1.5])\n",
    "init_utils.plot_decision_boundary(lambda x: init_utils.predict_dec(parameters, x.T), train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抑梯度异常初始化\n",
    "parameters = model(train_X, train_Y, initialization = \"he\",is_polt=True)\n",
    "print(\"训练集:\")\n",
    "predictions_train = init_utils.predict(train_X, train_Y, parameters)\n",
    "print(\"测试集:\")\n",
    "init_utils.predictions_test = init_utils.predict(test_X, test_Y, parameters)\n",
    "# 边界图\n",
    "plt.title(\"Model with He initialization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5, 1.5])\n",
    "axes.set_ylim([-1.5, 1.5])\n",
    "init_utils.plot_decision_boundary(lambda x: init_utils.predict_dec(parameters, x.T), train_X, train_Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正则化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y, test_X, test_Y = reg_utils.load_2D_dataset(is_plot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_with_dropout(X, parameters, keep_prob = 0.5):  # dropout的前向传播\n",
    "    np.random.seed(1)\n",
    "    w1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    w2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    w3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "  \n",
    "    Z1 = np.dot(w1, X) + b1\n",
    "    A1 = reg_utils.relu(Z1)\n",
    "    D1 = np.random.rand(A1.shape[0], A1.shape[1])\n",
    "    D1 = D1 < keep_prob\n",
    "    A1 = A1 * D1\n",
    "    A1 = A1 /  keep_prob\n",
    "    \n",
    "    Z2 = np.dot(w2, A1) + b2\n",
    "    A2 = reg_utils.relu(Z2)\n",
    "    D2 = np.random.rand(A2.shape[0], A2.shape[1])\n",
    "    D2 = D2 < keep_prob\n",
    "    A2 = A2 * D2\n",
    "    A2 = A2 / keep_prob\n",
    "    \n",
    "    Z3 = np.dot(w3, A2) + b3\n",
    "    A3 = reg_utils.sigmoid(Z3)\n",
    "    \n",
    "    cache = (Z1, D1, A1, w1, b1, Z2, D2, A2, w2, b2, Z3, A3, w3, b3)\n",
    "    return A3, cache\n",
    "\n",
    "def compute_cost_with_regularization(A_output, Y, parameters, lambd):  # L2正则化的loss\n",
    "    m = Y.shape[1]\n",
    "    w1 = parameters[\"W1\"]\n",
    "    w2 = parameters[\"W2\"]\n",
    "    w3 = parameters[\"W3\"]\n",
    "    cross_entropy_cost = reg_utils.compute_cost(A_output, Y)\n",
    "    L2_regularization_cost = (lambd / (2 * m)) * (np.sum(np.square(w1)) + np.sum(np.square(w2)) + np.sum(np.square(w2)))\n",
    "    loss = cross_entropy_cost + L2_regularization_cost\n",
    "    return loss\n",
    "\n",
    "def  backward_propagation_with_regularization(X, Y,cache, lambd): # L2正则化的反向传播,参数w和b保存在cache中\n",
    "    # 返回一次反向传播的梯度\n",
    "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    m = X.shape[1]\n",
    "    dZ3 = A3 - Y #起步是dz，交叉熵损失对Z3求导之后，经计算就是A3 - Y\n",
    "    dW3 = (1 / m) * np.dot(dZ3, A2.T) + (lambd / m) * W3\n",
    "    db3 = (1 / m) * np.sum(dZ3, axis = 1, keepdims = True)\n",
    "    \n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0 )) # A2 = relu(Z2),所以，当Z2>0时，导数是1\n",
    "    dW2 = (1 / m) * np.dot(dZ2, A1.T) + (lambd / m) * W2\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis = 1, keepdims = True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "    dW1 = (1 / m) * np.dot(dZ1, X.T) + (lambd / m) * W1\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis = 1, keepdims = True)\n",
    "    \n",
    "    grads = {\"dZ3\":dZ3, \"dW3\":dW3, \"db3\":db3,\n",
    "             \"dA2\":dA2, \"dZ2\":dZ2, \"dW2\":dW2, \"db2\":db2,\n",
    "             \"dA1\":dA1, \"dZ1\":dZ1, \"dW1\":dW1, \"db1\":db1\n",
    "            }\n",
    "    return grads\n",
    "def backward_propagation_with_dropout(X, Y, cache, keep_prob):\n",
    "    # 先正常前向传播，再用cache的D进行和前向传播同样的dropout处理\n",
    "    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    m = X.shape[1]\n",
    "    dz3 = A3 - Y #起步，由于loss的特殊性，计算后dz3 = A3 - Y\n",
    "    dw3 = (1 / m) * np.dot(dz3, A2.T)\n",
    "    db3 = (1 / m) * np.sum(dz3, axis = 1, keepdims = True)\n",
    "    \n",
    "    da2 = np.dot(W3.T, dz3)\n",
    "    da2 = da2 * D2\n",
    "    #da2 = da2 / (1 - keep_prob)\n",
    "    da2 = da2 / keep_prob\n",
    "    dz2 = np.multiply(da2, np.int64(A2 > 0)) \n",
    "    dw2 = (1 / m) * np.dot(dz2, A1.T)\n",
    "    db2 = (1 / m) * np.sum(dz2, axis = 1, keepdims = True)\n",
    "    \n",
    "    da1 = np.dot(W2.T, dz2)\n",
    "    da1 = da1 * D1\n",
    "    #da1 = da1 / (1 - keep_prob)\n",
    "    da1 = da1 / keep_prob\n",
    "    dz1 = np.multiply(da1, np.int64(A1 > 0))\n",
    "    dw1 = (1 / m) * np.dot(dz1, X.T)\n",
    "    db1 = (1 / m) * np.sum(dz1, axis = 1, keepdims = True)\n",
    "    \n",
    "    grads = {\"dZ3\":dz3, \"dW3\":dw3, \"db3\":db3,\n",
    "             \"dA2\":da2, \"dZ2\":dz2, \"dW2\":dw2, \"db2\":db2,\n",
    "             \"dA1\":da1, \"dZ1\":dz1, \"dW1\":dw1, \"db1\":db1    \n",
    "            }   \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, learning_rate = 0.3, num_iterations = 30000, print_cost = True, is_plot=True, lambd = 0, keep_prob = 1):\n",
    "    costs = []\n",
    "    layers_dims = [X.shape[0], 20, 3, 1]\n",
    "    parameters = reg_utils.initialize_parameters(layers_dims)\n",
    "    for i in range(num_iterations):\n",
    "        #前向传播\n",
    "        if keep_prob == 1:\n",
    "            A_output, cache = reg_utils.forward_propagation(X, parameters)\n",
    "        elif keep_prob < 1:\n",
    "            A_output, cache = forward_propagation_with_dropout(X, parameters, keep_prob)  # dropout\n",
    "        else :\n",
    "            print(\"keep_prob error\")\n",
    "            exit\n",
    "            \n",
    "        # 计算loss   \n",
    "        if lambd == 0:\n",
    "            cost = reg_utils.compute_cost(A_output, Y)\n",
    "        else:\n",
    "            cost = compute_cost_with_regularization(A_output, Y, parameters, lambd)\n",
    "            \n",
    "        # 反向传播\n",
    "        if (lambd == 0 and keep_prob == 1):\n",
    "            #不正则化，也不使用dropout\n",
    "            grads = reg_utils.backward_propagation(X, Y, cache)\n",
    "        elif lambd != 0:\n",
    "            # L2正则化\n",
    "            grads = backward_propagation_with_regularization(X, Y, cache, lambd)\n",
    "        elif keep_prob < 1:\n",
    "            # dropout正则化\n",
    "            grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)\n",
    "        \n",
    "        parameters = reg_utils.update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            costs.append(cost)\n",
    "            if print_cost:\n",
    "                print(\"iteration:{}\\t | cost: {}\".format(i, cost))\n",
    "                \n",
    "    if is_plot:\n",
    "        plt.plot(costs)\n",
    "        plt.ylabel(\"cost\")\n",
    "        plt.xlabel(\"iterations\")\n",
    "        plt.title(\"learning rate :\" + str(learning_rate))\n",
    "        plt.show()\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 测试不使用正则化\n",
    "# parameters = model(train_X, train_Y,is_plot=True)\n",
    "# print(\"训练集:\")\n",
    "# predictions_train = reg_utils.predict(train_X, train_Y, parameters)\n",
    "# print(\"测试集:\")\n",
    "# predictions_test = reg_utils.predict(test_X, test_Y, parameters)\n",
    "# # 边界图\n",
    "# plt.title(\"Model without regularization\")\n",
    "# axes = plt.gca()\n",
    "# axes.set_xlim([-0.75,0.40])\n",
    "# axes.set_ylim([-0.75,0.65])\n",
    "# reg_utils.plot_decision_boundary(lambda x: reg_utils.predict_dec(parameters, x.T), train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 测试使用正则化\n",
    "# parameters = model(train_X, train_Y, lambd=0.7,is_plot=True)\n",
    "# print(\"使用正则化，训练集:\")\n",
    "# predictions_train = reg_utils.predict(train_X, train_Y, parameters)\n",
    "# print(\"使用正则化，测试集:\")\n",
    "# predictions_test = reg_utils.predict(test_X, test_Y, parameters)\n",
    "# # 画边界线\n",
    "# plt.title(\"Model with L2-regularization\")\n",
    "# axes = plt.gca()\n",
    "# axes.set_xlim([-0.75,0.40])\n",
    "# axes.set_ylim([-0.75,0.65])\n",
    "# reg_utils.plot_decision_boundary(lambda x: reg_utils.predict_dec(parameters, x.T), train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #测试使用dropout\n",
    "# parameters = model(train_X, train_Y, learning_rate=0.3, is_plot=True, keep_prob=0.86)\n",
    "\n",
    "# print(\"使用随机删除节点，训练集:\")\n",
    "# predictions_train = reg_utils.predict(train_X, train_Y, parameters)\n",
    "# print(\"使用随机删除节点，测试集:\")\n",
    "# reg_utils.predictions_test = reg_utils.predict(test_X, test_Y, parameters)\n",
    "# # 画出决策边界\n",
    "# plt.title(\"Model with dropout\")\n",
    "# axes = plt.gca()\n",
    "# axes.set_xlim([-0.75, 0.40])\n",
    "# axes.set_ylim([-0.75, 0.65])\n",
    "# reg_utils.plot_decision_boundary(lambda x: reg_utils.predict_dec(parameters, x.T), train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_n(X, Y, parameters):\n",
    "    # 前向传播并计算loss\n",
    "    m = X.shape[1]\n",
    "    w1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    w2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    w3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    z1 = np.dot(w1, X) + b1\n",
    "    a1 = reg_utils.relu(z1)\n",
    "    z2 = np.dot(w2, a1) + b2\n",
    "    a2 = reg_utils.relu(z2)\n",
    "    z3 = np.dot(w3, a2) + b3\n",
    "    a3 = reg_utils.sigmoid(z3)\n",
    "    \n",
    "    cost = -(1 / m) *np.sum( Y * np.log(a3) + (1 - Y) * np.log(1 - a3))\n",
    "    cache = (z1, a1, w1, b1, z2, a2, w2, b2, z3, a3, w3, b3)\n",
    "    return cost,cache\n",
    "\n",
    "def backward_propagation_n(X, Y, cache):\n",
    "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    m = X.shape[1]\n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = (1 / m) * np.dot(dZ3, A2.T)\n",
    "    db3 = (1 / m) * np.sum(dZ3, axis = 1, keepdims = True)\n",
    "    \n",
    "    dA2 = np.dot(W3.T,dZ3)\n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis = 1, keepdims = True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "    dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis = 1, keepdims = True)\n",
    "    \n",
    "    grads = {\"dZ3\":dZ3, \"dW3\":dW3, \"db3\":db3,\n",
    "             \"dA2\":dA2, \"dZ2\":dZ2, \"dW2\":dW2, \"db2\":db2,\n",
    "             \"dA1\":dA1, \"dZ1\":dZ1, \"dW1\":dW1, \"db1\":db1        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    return grads\n",
    "\n",
    "def gradient_check_n(parameters, gradients, X, Y, epsilon = 1e-7):\n",
    "    # 设置变量\n",
    "    parameters_values, _ = gc_utils.dictionary_to_vector(parameters)  # 将参数变成向量\n",
    "    grad = gc_utils.gradients_to_vector(gradients)\n",
    "    num_parameters = parameters_values.shape[0]\n",
    "    J_plus = np.zeros((num_parameters, 1))\n",
    "    J_minus = np.zeros((num_parameters, 1))\n",
    "    gradapprox = np.zeros((num_parameters, 1))\n",
    "    \n",
    "    # 计算gradaprox\n",
    "    for i in range(num_parameters):\n",
    "        thetaplus = np.copy(parameters_values)\n",
    "        thetaplus[i][0] = thetaplus[i][0] + epsilon\n",
    "        J_plus[i], cache = forward_propagation_n(X, Y, gc_utils.vector_to_dictionary(thetaplus))  # 只改变了第i个参数，求出J(theta[i] + epsilon)\n",
    "        \n",
    "        thetaminus = np.copy(parameters_values)\n",
    "        thetaminus[i][0] = thetaminus[i][0] - epsilon\n",
    "        J_minus[i],_ = forward_propagation_n(X, Y, gc_utils.vector_to_dictionary(thetaminus))\n",
    "        \n",
    "        gradapprox[i] = (J_plus[i] - J_minus[i]) / (2 * epsilon)\n",
    "        \n",
    "    \n",
    "    numerator = np.linalg.norm(grad - gradapprox)\n",
    "    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)\n",
    "    difference = numerator / denominator\n",
    "    \n",
    "    if difference < 1e-7:\n",
    "        print(\"gradients normal, difference: {}\".format(difference))\n",
    "    else :\n",
    "        print(\"gradients error, difference: {}\".format(difference))\n",
    "        \n",
    "    return difference\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, parameters = testCases.gradient_check_n_test_case()\n",
    "\n",
    "cost, cache = forward_propagation_n(X, Y, parameters)\n",
    "gradients = backward_propagation_n(X, Y, cache)\n",
    "difference = gradient_check_n(parameters, gradients, X, Y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_exercise",
   "language": "python",
   "name": "dl_py36_study"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
