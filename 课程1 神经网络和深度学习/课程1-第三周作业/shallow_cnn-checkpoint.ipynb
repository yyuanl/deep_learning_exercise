{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入软件包\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from testCases import *\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "from planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_planar_dataset()  #加载数据\n",
    "plt.scatter(X[0, :], X[1, :], c = np.squeeze(Y), s = 40, cmap = plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_X = X.shape\n",
    "shape_Y = Y.shape\n",
    "m = shape_X[1]\n",
    "print('The size of X is : {}\\nThe size of Y is : {}\\nThe number of datasets is : {}'.format(shape_X, shape_Y, m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = sklearn.linear_model.LogisticRegressionCV() #先使用逻辑回归看看\n",
    "clf.fit(X.T, Y.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(lambda x: clf.predict(x), X, np.squeeze(Y))# 画决策边界\n",
    "plt.title(\"Logistic Regression\")\n",
    "LR_predictions = clf.predict(X.T) #预测结果\n",
    "print('The accuracy of LR :{}%'.format(float((np.dot(Y, LR_predictions) + \n",
    "                                             np.dot(1 - Y,1 - LR_predictions)) / float(Y.size) * 100)))\n",
    "#预测正确的：1.预测1正确 2.预测0正确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(input_size, hidden_layer_size, output_size):\n",
    "    np.random.seed(2)\n",
    "    w1 = np.random.randn(hidden_layer_size, input_size)*0.01\n",
    "    b1 = np.zeros((hidden_layer_size,1))\n",
    "    w2 = np.random.randn(output_size, hidden_layer_size)*0.01\n",
    "    b2 = np.zeros((output_size, 1))\n",
    "    init_params = {\"w1\":w1, \"w2\":w2, \"b1\":b1, \"b2\":b2}\n",
    "    return init_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x , n_h , n_y = initialize_parameters_test_case()\n",
    "parameters = init_params(n_x , n_h , n_y)\n",
    "print(\"W1 = \" + str(parameters[\"w1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"w2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    sig = np.zeros_like(z)\n",
    "    sig = 1/(1 + np.exp(-z))\n",
    "    return sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwa_propa(input, params):\n",
    "    \"\"\"\n",
    "    定义params = {\"w1\":w1, \"w2\":w2, \"b1\":b1, \"b2\":b2}\n",
    "    train_x:每列代表一个数据\n",
    "    train_y:行向量\n",
    "    \"\"\"\n",
    "    m = input.shape[1]\n",
    "    w1 = params[\"w1\"]\n",
    "    w2 = params[\"w2\"]\n",
    "    b1 = params[\"b1\"]\n",
    "    b2 = params[\"b2\"]\n",
    "    \n",
    "    a0 = input #输入成\n",
    "    \n",
    "    # 隐藏层\n",
    "    z1 = np.dot(w1, a0) + b1 # 用矩阵w1对每个数据进行线性变换，得到第二层输入值\n",
    "    a1 = np.tanh(z1) #矩阵：第i列是第i个样本在隐藏层的激活值\n",
    "    # 输出层\n",
    "    z2 = np.dot(w2, a1) + b2 \n",
    "    a2 = sigmoid(z2) # 1 x m的行向量（输出层只有一个神经元）\n",
    "    \n",
    "    \n",
    "    cache = {\"z1\":z1,\"a1\":a1,\"z2\":z2, \"a2\":a2}\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcu_loss(h_x, train_y):\n",
    "    a2 = h_x\n",
    "    m = train_y.shape[1]\n",
    "    loss = -(1/m)*(np.dot(train_y, np.log(a2.T)) + np.dot(1 - train_y, np.log(1 - a2.T)))\n",
    "    loss = float(np.squeeze(loss))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propa(train_x, train_y, a, params, rate):#单次反向传播\n",
    "    \"\"\"\n",
    "    a：向前传播的没错激活值，a = {'a1':a1, 'a2':a2}\n",
    "    返回一次优化后的参数：params = {\"w1\":w1, \"w2\":w2, \"b1\":b1, \"b2\":b2}\n",
    "    \"\"\"\n",
    "    #print('--before back propagation :', params['w1'][:, 1:10])\n",
    "    m = train_y.shape[1]\n",
    "    a0 = train_x\n",
    "    a1 = a[\"a1\"] # 1xm\n",
    "    a2 = a[\"a2\"] # 4xm\n",
    "    w1 = params[\"w1\"]\n",
    "    w2 = params[\"w2\"]\n",
    "    #反向传播计算各层参数的导数矩阵\n",
    "    delat2 = a2 - train_y  #第二层误差,行向量，每列是每个样本的delat2\n",
    "    delat1 = np.dot(w2.T, delat2)*(1 - np.power(a1, 2)) # 对每个样本的delat2做Ww2.T的线性变换后...每列代表每个样本的delat1值   \n",
    "    dw2 = 1/m * np.dot(delat2, a1.T)  # 列乘以行得到参数偏导矩阵，m个相加\n",
    "    db2 = 1/m * np.sum(delat2, axis = 1, keepdims = True)\n",
    "    dw1 = 1/m * np.dot(delat1, a0.T)\n",
    "    db1 = 1/m * np.sum(delat1, axis = 1, keepdims = True)\n",
    "    assert(dw2.shape == (a2.shape[0], a1.shape[0]))\n",
    "    \n",
    "    delat = {\"dw2\":dw2, \"dw1\":dw1, \"db1\":db1, \"db2\":db2}\n",
    "    \n",
    "    params['w1'] = params['w1'] - rate * dw1\n",
    "    params['b1'] = params['b1'] - rate * db1\n",
    "    params['w2'] = params['w2'] - rate * dw2\n",
    "    params['b2'] = params['b2'] - rate * db2\n",
    "    #print('++after back propagation :', params['w1'][:, 1:10])\n",
    "    return (params,delat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=========================测试backward_propagation=========================\")\n",
    "parameters, cache, X_assess, Y_assess = backward_propagation_test_case()\n",
    "params = {'w1':parameters[\"W1\"], \"w2\":parameters[\"W2\"],\"b1\":parameters[\"b1\"], \"b2\":parameters[\"b2\"]}\n",
    "cache = {\"a1\":cache[\"A1\"], \"a2\":cache[\"A2\"]}\n",
    "\n",
    "_, grads = back_propa(X_assess, Y_assess, cache, params, rate = 0.5)\n",
    "print (\"dW1 = \"+ str(grads[\"dw1\"]))\n",
    "print (\"db1 = \"+ str(grads[\"db1\"]))\n",
    "print (\"dW2 = \"+ str(grads[\"dw2\"]))\n",
    "print (\"db2 = \"+ str(grads[\"db2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_x, train_y, iterations = 10000, print_loss = False, rate = 0.5, hidden_layer_size = 4):\n",
    "    \"\"\"\n",
    "    输入训练集、学习率、初始化的参数，经过梯度下降得到优化后的参数 \n",
    "    \"\"\"\n",
    "    np.random.seed(3)\n",
    "    cost = []\n",
    "    params = init_params(train_x.shape[0], hidden_layer_size, 1)\n",
    "    print('{}Train start{}'.format('.' * 5, '.' * 5))\n",
    "    for i in range(iterations):\n",
    "        #1.正向传播，得到当前更新参数后的loss，要看看在当前参数下的loss，得到当前各层激活值。\n",
    "        cache = forwa_propa(train_x, params)\n",
    "        #print('*'*10)\n",
    "        los = calcu_loss(cache[\"a2\"], train_y)\n",
    "        if i % 1000 == 0:\n",
    "            cost.append(los)\n",
    "            #print('-'*10)\n",
    "        if print_loss and i % 1000 ==0:\n",
    "            print('Iteration: {} | loss: {}'.format(i, los))\n",
    "        #2.gradient descent更新参数\n",
    "        params,_ = back_propa(train_x, train_y, cache, params, rate)\n",
    "    #返回最终的参数，和loss（记录训练过程中的loss）\n",
    "    print('{}Train end{}'.format('.' * 5, '.' * 5))\n",
    "    return (params, cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=========================测试nn_model=========================\")\n",
    "X_assess, Y_assess = nn_model_test_case()\n",
    "print('The shape of X_assess :', X_assess.shape)\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "\n",
    "parameters, _ = train(X_assess, Y_assess, iterations=10000, print_loss=False, rate = 0.5, hidden_layer_size = 4)\n",
    "print(\"W1 = \" + str(parameters[\"w1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"w2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_train, cost = train(X, Y, iterations = 10000, print_loss = True, rate = 0.5, hidden_layer_size = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(params, X):\n",
    "    cache = forwa_propa(X, params)\n",
    "    pred = np.round(cache[\"a2\"])\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=========================测试predict=========================\")\n",
    "\n",
    "parameters, X_assess = predict_test_case()\n",
    "params_test = {'w1':parameters[\"W1\"], \"w2\":parameters[\"W2\"],\"b1\":parameters[\"b1\"], \"b2\":parameters[\"b2\"]}\n",
    "\n",
    "predictions = predict(params_test, X_assess)\n",
    "print(\"预测的平均值 = \" + str(np.mean(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(lambda x: predict(params_train, x.T), X, np.squeeze(Y))  \n",
    "plt.title(\"Decision Boundary for hidden layer size \" + str(4))\n",
    "predictions = predict(params_train, X)\n",
    "accua = float((np.dot(Y, predictions.T) + np.dot(1 - Y, 1 - predictions.T))/float(Y.size)) * 100\n",
    "print(\"accuracy:{}%\".format(accua))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,32))\n",
    "hidden_size = [1, 2, 3, 4, 5, 20, 50]\n",
    "for index, num_hidd in enumerate(hidden_size):\n",
    "    plt.subplot(5, 2, index + 1)\n",
    "    plt.title('Hidden Layer of size {}'.format(num_hidd))\n",
    "    params_train, _ = train(X, Y, iterations = 5000, print_loss = False, rate = 0.5, hidden_layer_size = num_hidd)\n",
    "    plot_decision_boundary(lambda x: predict(params_train, x.T), X, np.squeeze(Y))    \n",
    "    predictions = predict(params_train, X)\n",
    "    accuracy = float((np.dot(Y, predictions.T) + np.dot(Y, predictions.T))/float(Y.size) * 100)\n",
    "    print('the number of hidden layer is {}, the accuracy is {}%'.format(num_hidd, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6(py36)",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
