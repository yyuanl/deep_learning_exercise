{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import testCases\n",
    "from dnn_utils import sigmoid, sigmoid_backward,relu, relu_backward\n",
    "import lr_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化参数\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    w1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    w2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    \n",
    "    assert(w1.shape == (n_h, n_x))\n",
    "    parameters = {\"W1\":w1, \"W2\":w2, \"b1\":b1, \"b2\":b2}\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"==============测试initialize_parameters==============\")\n",
    "# parameters = initialize_parameters(3,2,1)\n",
    "# print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "# print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "# print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "# print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化深层网络参数\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    layer_dims:列表，每层神经元个数，包括输入层\n",
    "    \"\"\"\n",
    "    for index in range(1, len(layer_dims)):\n",
    "        parameters[\"W\" + str(index)] = np.random.randn(layer_dims[index], layer_dims[index - 1]) / np.sqrt(layer_dims[index - 1])\n",
    "        parameters[\"b\" + str(index)] = np.zeros((layer_dims[index], 1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #测试initialize_parameters_deep\n",
    "# print(\"==============测试initialize_parameters_deep==============\")\n",
    "# layers_dims = [5,4,3]\n",
    "# parameters = initialize_parameters_deep(layers_dims)\n",
    "# print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "# print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "# print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "# print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    Z = np.dot(W, A) + b\n",
    "    assert(Z.shape == (W.shape[0], A.shape[1])) #考虑多个样本\n",
    "    cache = (A, W, b) # A：上一层激活值，计算delta和dW,W:用来反向计算delta\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============测试linear_forward==============\n",
      "Z = [[ 3.26295337 -1.23429987]]\n"
     ]
    }
   ],
   "source": [
    "# #测试linear_forward\n",
    "# print(\"==============测试linear_forward==============\")\n",
    "# A,W,b = testCases.linear_forward_test_case()\n",
    "# Z,linear_cache = linear_forward(A,W,b)\n",
    "# print(\"Z = \" + str(Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============测试linear_activation_forward==============\n",
      "sigmoid，A = [[0.96890023 0.11013289]]\n",
      "ReLU，A = [[3.43896131 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# #测试linear_activation_forward\n",
    "# print(\"==============测试linear_activation_forward==============\")\n",
    "# A_prev, W,b = testCases.linear_activation_forward_test_case()\n",
    "\n",
    "# A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
    "# print(\"sigmoid，A = \" + str(A))\n",
    "\n",
    "# A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
    "# print(\"ReLU，A = \" + str(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6(py36)",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
