{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import testCases\n",
    "from dnn_utils import sigmoid, sigmoid_backward,relu, relu_backward\n",
    "import lr_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化参数\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    w1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    w2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    \n",
    "    assert(w1.shape == (n_h, n_x))\n",
    "    parameters = {\"W1\":w1, \"W2\":w2, \"b1\":b1, \"b2\":b2}\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"==============测试initialize_parameters==============\")\n",
    "# parameters = initialize_parameters(3,2,1)\n",
    "# print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "# print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "# print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "# print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化深层网络参数\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    layer_dims:列表，每层神经元个数，包括输入层\n",
    "    \"\"\"\n",
    "    for index in range(1, len(layer_dims)):\n",
    "        parameters[\"W\" + str(index)] = np.random.randn(layer_dims[index], layer_dims[index - 1]) / np.sqrt(layer_dims[index - 1])\n",
    "        parameters[\"b\" + str(index)] = np.zeros((layer_dims[index], 1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #测试initialize_parameters_deep\n",
    "# print(\"==============测试initialize_parameters_deep==============\")\n",
    "# layers_dims = [5,4,3]\n",
    "# parameters = initialize_parameters_deep(layers_dims)\n",
    "# print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "# print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "# print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "# print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "#     print(\"w.shape:\", W.shape)\n",
    "#     print(\"A.shape:\", A.shape)    \n",
    "    Z = np.dot(W, A) + b\n",
    "    assert(Z.shape == (W.shape[0], A.shape[1])) #考虑多个样本\n",
    "    cache = (A, W, b) # A：上一层激活值，计算delta和dW,W:用来反向计算delta\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #测试linear_forward\n",
    "# print(\"==============测试linear_forward==============\")\n",
    "# A,W,b = testCases.linear_forward_test_case()\n",
    "# Z,linear_cache = linear_forward(A,W,b)\n",
    "# print(\"Z = \" + str(Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):  # 前向传播最关键的函数\n",
    "    \"\"\"\n",
    "    记录上一层的激活值A，本层的W、b、A。linear_cache:Al-1、W、b; activation_cache;Al\n",
    "    \"\"\"\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #测试linear_activation_forward\n",
    "# print(\"==============测试linear_activation_forward==============\")\n",
    "# A_prev, W,b = testCases.linear_activation_forward_test_case()\n",
    "\n",
    "# A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
    "# print(\"sigmoid，A = \" + str(A))\n",
    "\n",
    "# A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
    "# print(\"ReLU，A = \" + str(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L层模型的前向传播\n",
    "def L_model_forward(X, parameters):\n",
    "#     print(\"X:\", X[1:5, 1:5])\n",
    "#     print(\"parameters\", parameters[\"W1\"][1:3,1:4])\n",
    "#     \"\"\"\n",
    "#     参数决定了网络的形状,最终返回所有样本的输出值，和所有层的缓存\n",
    "#     \"\"\"\n",
    "    caches = []    #列表，记录每层的linear_cache：Al-1、W、b和activation_cache：Al，每个元素是一个元组\n",
    "    A = X\n",
    "    num_layer = len(parameters) // 2\n",
    "    # 1到num_layer - 1层都是relu激活\n",
    "    for index in range(1, num_layer):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters[\"W\" + str(index)], parameters[\"b\" + str(index)], activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "    # num_layer层sigmodi激活\n",
    "    A_output, cache = linear_activation_forward(A, parameters[\"W\" + str(num_layer)], parameters[\"b\" + str(num_layer)], activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    #assert(A_output.shape == (len(patameters[\"b\" + str(num_layer)])), X.shape[1])\n",
    "    return A_output, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试L_model_forward\n",
    "print(\"==============测试L_model_forward==============\")\n",
    "X,parameters = testCases.L_model_forward_test_case()\n",
    "AL,caches = L_model_forward(X,parameters)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"caches 的长度为 = \" + str(len(caches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A_output, Y):\n",
    "    m = Y.shape[1]\n",
    "    cost = -(1 / m) * (np.dot(Y, np.log(A_output.T)) + np.dot(1 - Y, np.log(1 - A_output.T)))\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #测试compute_cost\n",
    "# print(\"==============测试compute_cost==============\")\n",
    "# Y,AL = testCases.compute_cost_test_case()\n",
    "# print(\"cost = \" + str(compute_cost(AL, Y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, linear_cache):    \n",
    "    A_prev, W, b = linear_cache\n",
    "    m = dZ.shape[1]  \n",
    "    dW = (1 / m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1 / m) * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    assert(dW.shape == W.shape)\n",
    "    assert(db.shape == b.shape)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #测试linear_backward\n",
    "# print(\"==============测试linear_backward==============\")\n",
    "# dZ, linear_cache = testCases.linear_backward_test_case()\n",
    "\n",
    "# dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "# print (\"dA_prev = \"+ str(dA_prev))\n",
    "# print (\"dW = \" + str(dW))\n",
    "# print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation = \"relu\"):   # 后向传播最关键的函数,从这里入手写\n",
    "    \"\"\"\n",
    "    最终要通过当前L层的dA和cache（W,b,A_prev,A）计算出上一层L - 1的dA_prev\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache #activation_cache实际上就是A\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)        \n",
    "    return dA_prev, dW, db "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总结：\n",
    "对于每层来说， 两个函数操作，一个是线性组合，一个激活。linear_cache存的是线性组合的缓存（偏导），包括上一层的激活值，本层的参数。activation存的是激活值的缓存（偏导）。cache包括这两个缓存值。使用linear_activation_backward(dA, cache, activation = \"relu\")函数，计算上一层激活值得偏导dA_prev,计算本层的dW、db，具体分为两步计算，dZ = relu_backward(dA, activation_cache)和dA_prev, dW, db = linear_backward(dZ, linear_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #测试linear_activation_backward\n",
    "# print(\"==============测试linear_activation_backward==============\")\n",
    "# AL, linear_activation_cache = testCases.linear_activation_backward_test_case()\n",
    "\n",
    "# dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = \"sigmoid\")\n",
    "# print (\"sigmoid:\")\n",
    "# print (\"dA_prev = \"+ str(dA_prev))\n",
    "# print (\"dW = \" + str(dW))\n",
    "# print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "# dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = \"relu\")\n",
    "# print (\"relu:\")\n",
    "# print (\"dA_prev = \"+ str(dA_prev))\n",
    "# print (\"dW = \" + str(dW))\n",
    "# print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(A_output, Y, caches):\n",
    "    \"\"\"\n",
    "    A_outpu:最终的输出，m列\n",
    "    Y：m列\n",
    "    caches：列表，共L个元素，每个代表某层的缓存（两部分）\n",
    "    返回：grads列表，每层参数的偏导值，dA、dW、db\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    num_layers = len(caches)\n",
    "    \n",
    "    #先计算最后一层\n",
    "    dA_output = - (np.divide(Y, A_output) - np.divide(1 - Y, 1 - A_output))\n",
    "    cache_output_layer = caches[num_layers - 1]\n",
    "    dA_prev_layer, dW_output, db_output = linear_activation_backward(dA_output, cache_output_layer, activation = \"sigmoid\")#dA_prev_layer：倒数第二层激活值的偏导\n",
    "    \n",
    "    grads[\"dA\" + str(num_layers)] = dA_output\n",
    "    grads[\"dW\" + str(num_layers)] = dW_output\n",
    "    grads[\"db\" + str(num_layers)] = db_output\n",
    "    \n",
    "    dA_index_layer = dA_prev_layer\n",
    "    for index in range(1, num_layers):\n",
    "        #计算当前层参数的偏导数和上一层激活值的偏导数\n",
    "        dA_prev_layer, dW_index_layer, db_index_layer = linear_activation_backward(dA_index_layer, caches[num_layers - index - 1], activation = \"relu\")\n",
    "        \n",
    "        grads[\"dA\" + str(num_layers - index)] = dA_index_layer\n",
    "        grads[\"dW\" + str(num_layers - index)] = dW_index_layer\n",
    "        grads[\"db\" + str(num_layers - index)] = db_index_layer\n",
    "        \n",
    "        dA_index_layer = dA_prev_layer\n",
    "    return grads\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试L_model_backward\n",
    "print(\"==============测试L_model_backward==============\")\n",
    "AL, Y_assess, caches = testCases.L_model_backward_test_case()\n",
    "grads = L_model_backward(AL, Y_assess, caches)\n",
    "print (\"dW1 = \"+ str(grads[\"dW1\"]))\n",
    "print (\"db1 = \"+ str(grads[\"db1\"]))\n",
    "print (\"dA1 = \"+ str(grads[\"dA1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    num_params = len(parameters) // 2\n",
    "    for index in range(1,num_params + 1):\n",
    "        parameters[\"W\" + str(index)] = parameters[\"W\" + str(index)] - learning_rate * grads[\"dW\" + str(index)]\n",
    "        parameters[\"b\" + str(index)] = parameters[\"b\" + str(index)] - learning_rate * grads[\"db\" + str(index)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试update_parameters\n",
    "print(\"==============测试update_parameters==============\")\n",
    "parameters, grads = testCases.update_parameters_test_case()\n",
    "parameters = update_parameters(parameters, grads, 0.1)\n",
    "\n",
    "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
    "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
    "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
    "print (\"b2 = \"+ str(parameters[\"b2\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#搭建两层神经网络\n",
    "def two_layer_model(X,Y,layers_dims,learning_rate=0.0075,num_iterations=120,print_cost=False,isPlot=True):\n",
    "    \"\"\"\n",
    "    丢入数据集、网络大小、学习率、迭代次数等，就能返回更新后的参数\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    (n_x, n_h, n_y) = layers_dims\n",
    "    grads = {}\n",
    "    costs = []\n",
    "    print('.........................................')\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    \n",
    "    print(\"train start...\")\n",
    "    for index in range(num_iterations):\n",
    "        W1 = parameters[\"W1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        A1, cache1 = linear_activation_forward(X, W1, b1,activation = \"relu\")\n",
    "        A_output, cache2 = linear_activation_forward(A1, W2, b2, activation = \"sigmoid\" )\n",
    "        \n",
    "        cost = compute_cost(A_output, Y)\n",
    "        \n",
    "        dA_output = - (np.divide(Y, A_output) - np.divide(1 - Y, 1 - A_output))\n",
    "        dA_prev_layer, dW_output, db_output = linear_activation_backward(dA_output, cache2, activation = \"sigmoid\")\n",
    "        dA_prev_layer, dW_1, db_1 = linear_activation_backward(dA_prev_layer, cache1, activation = \"relu\")\n",
    "        \n",
    "        grads[\"dW2\"] = dW_output\n",
    "        grads[\"dW1\"] = dW_1\n",
    "        grads[\"db1\"] = db_1\n",
    "        grads[\"db2\"] = db_output\n",
    "        \n",
    "        parameters = update_parameters(parameters, grads, learning_rate) #更新参数\n",
    "        \n",
    "        if index % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            if print_cost:\n",
    "                print(\"Iteration: {} | cost: {}\".format(index, np.squeeze(cost)))\n",
    "    print(\"train end...\")           \n",
    "    if isPlot:\n",
    "        plt.plot(np.squeeze(costs))\n",
    "#         plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "train_set_x_orig , train_set_y , test_set_x_orig , test_set_y , classes = lr_utils.load_dataset()\n",
    "\n",
    "train_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T \n",
    "test_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\n",
    "print(train_x_flatten.shape)\n",
    "train_x = train_x_flatten / 255.0\n",
    "train_y = train_set_y\n",
    "test_x = test_x_flatten / 255.0\n",
    "test_y = test_set_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = (12288, 7, 1)\n",
    "parameters = two_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost=True,isPlot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    \n",
    "    A_output, caches = L_model_forward(X, parameters)\n",
    "    pred = np.zeros_like(A_output)\n",
    "    \n",
    "    for output_index in range(A_output.shape[1]):\n",
    "        if A_output[0, output_index] > 0.5: \n",
    "            pred[0, output_index] = 1\n",
    "        else:\n",
    "            pred[0, output_index] = 0\n",
    "    accuracy = float(np.sum(pred == y) / pred.shape[1])\n",
    "    print('The accuracy is {}%'.format(accuracy * 100))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = predict(train_x, train_y, parameters) #训练集\n",
    "predictions_test = predict(test_x, test_y, parameters) #测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False,isPlot=True):\n",
    "    np.random.seed(1)\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    costs = []\n",
    "    print('start train ...')\n",
    "    for i in range(num_iterations):\n",
    "        A_output, caches = L_model_forward(X, parameters)\n",
    "        if i == 0:\n",
    "            print('A_output:',A_output[0, 1:5])\n",
    "        cost = compute_cost(A_output, Y)\n",
    "\n",
    "        grads = L_model_backward(A_output, Y, caches)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            if  print_cost:\n",
    "                print('Iterations:{}\\t| cost:{}'.format(i, cost))\n",
    "    \n",
    "    print('finish train ...')\n",
    "    if isPlot:\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations per 100')\n",
    "        plt.title(\"Learning rate = \" + str(learning_rate))\n",
    "        plt.show\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x_orig , train_set_y , test_set_x_orig , test_set_y , classes = lr_utils.load_dataset()\n",
    "\n",
    "train_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T \n",
    "test_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\n",
    "\n",
    "train_x = train_x_flatten / 255\n",
    "train_y = train_set_y\n",
    "test_x = test_x_flatten / 255\n",
    "test_y = test_set_y\n",
    "\n",
    "layers_dims = [12288, 20, 7, 5, 1] #  5-layer model\n",
    "parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True,isPlot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = predict(train_x, train_y, parameters) #训练集\n",
    "pred_test = predict(test_x, test_y, parameters) #测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mislabeled_images(classes, X, y, p):\n",
    "    \"\"\"\n",
    "    绘制预测和实际不同的图像。\n",
    "        X - 数据集\n",
    "        y - 实际的标签\n",
    "        p - 预测\n",
    "    \"\"\"\n",
    "    a = p + y\n",
    "    mislabeled_indices = np.asarray(np.where(a == 1))\n",
    "    plt.rcParams['figure.figsize'] = (40.0, 40.0) # set default size of plots\n",
    "    num_images = len(mislabeled_indices[0])\n",
    "    for i in range(num_images):\n",
    "        index = mislabeled_indices[1][i]\n",
    "\n",
    "        plt.subplot(2, num_images, i + 1)\n",
    "        plt.imshow(X[:,index].reshape(64,64,3), interpolation='nearest')\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Prediction: \" + classes[int(p[0,index])].decode(\"utf-8\") + \" \\n Class: \" + classes[y[0,index]].decode(\"utf-8\"))\n",
    "\n",
    "\n",
    "print_mislabeled_images(classes, test_x, test_y, pred_test)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6(py36)",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
